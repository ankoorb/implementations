{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1a8bec-debd-4a33-a54b-6f4dffbde96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be78bf5d-635f-4684-b46e-e23843b6b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "class Sampler:\n",
    "    \"\"\"\n",
    "    Sampler base class\n",
    "    \"\"\"\n",
    "    def __call__(self, logits):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202f49f-9a4f-48e7-aace-18ac1d4c7233",
   "metadata": {},
   "source": [
    "# Greedy Sampling\n",
    "\n",
    "Greedy sampling (also called greedy decoding) selects the most probable next token at each step in the sequence generation â€” no randomness, just the top choice every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74ad639-fa71-49b7-ac2c-6911bcf41140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySampler(Sampler):\n",
    "    def __call__(self, logits):\n",
    "        return logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ecca9b-7e53-488f-a23c-308cb3dcb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token = 'f'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sampler = GreedySampler()\n",
    "vocabulary = list(set(\"abcdefghijklmnopqrstuvwxyz.\"))\n",
    "logits = torch.randn(1, 27)\n",
    "next_token_id = sampler(logits)\n",
    "next_token = vocabulary[next_token_id.item()]\n",
    "print(f\"{next_token = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc6dbd-35ab-4739-a603-2e01fea6267c",
   "metadata": {},
   "source": [
    "# Temperature Sampling\n",
    "\n",
    "Temperature sampling is a technique used in language model sampling to control the randomness (or confidence) of token selection by scaling the logits before applying softmax.\n",
    "\n",
    "$$\n",
    "\\text{scaled logits}_i = \\frac{u_i}{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_i = V_l \\mid x_{1:i-1}) = \\frac{\\exp\\left( \\frac{u_l}{T} \\right)}{\\sum_{j} \\exp\\left( \\frac{u_j}{T} \\right)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $u_j$: Logit for token $j$\n",
    "- $T$: Temperature\n",
    "- $V_l$: Vocabulary token at index $l$\n",
    "- $P(x_i = V_l \\mid x_{1:i-1})$ is probability that the next token is vocabulary token $V_l$, given the sequence so far.\n",
    "\n",
    "---\n",
    "\n",
    "When $\\text{temperature} = 0$ the sampling is treated as greedy sampling.\n",
    "\n",
    "As $T \\to 0$, the scaled logits $\\frac{u_i}{T}$ become very large for the maximum logit $u_{max}$ and relatively much smaller for others. This causes the softmax distribution to concentrate all probability mass on the token with the highest logit:\n",
    "\n",
    "$$\n",
    "\\lim_{T \\to 0} \\text{softmax}\\left(\\frac{u}{T}\\right) = \\text{one-hot vector at } \\arg\\max_i u_i\n",
    "$$\n",
    "\n",
    "Therefore, sampling from this distribution is equivalent to choosing the token with the highest logit, which is exactly greedy sampling. In practice, since dividing by zero is undefined and numerically unstable, return the $\\arg\\max$ when $T = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a239975e-10ec-4802-9122-426f267f91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureSampler(Sampler):\n",
    "    def __init__(self, temperature=1):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, logits):\n",
    "        if self.temperature == 0:\n",
    "            # Greedy sampling\n",
    "            return logits.argmax(dim=-1)\n",
    "\n",
    "        # Scale logits\n",
    "        scaled_logits = logits / self.temperature\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        # Sample from the probability distribution\n",
    "        return torch.multinomial(probs, num_samples=1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b18fcf7-ccf7-4dff-b04c-de2509ace0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token = 'b'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sampler = TemperatureSampler(temperature=5)\n",
    "vocabulary = list(set(\"abcdefghijklmnopqrstuvwxyz.\"))\n",
    "logits = torch.randn(1, 27)\n",
    "next_token_id = sampler(logits)\n",
    "next_token = vocabulary[next_token_id.item()]\n",
    "print(f\"{next_token = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b3f1e-12e4-48da-ba58-03f9e7078f6c",
   "metadata": {},
   "source": [
    "# Top-K Sampling\n",
    "\n",
    "Top-k sampling limits the candidate tokens to the k highest probability tokens. It sets probabilities of all other tokens to zero. Then samples the next token only from these top-k tokens. This controls randomness by restricting to a smaller subset of likely tokens.\n",
    "\n",
    "Top-k sampling can be combined with Temperature Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d795ab9f-fc26-417e-babf-2a132bd5c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKSampler(Sampler):\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, logits):\n",
    "        # Get top-k logits and their indices\n",
    "        topk_logits, topk_indices = torch.topk(logits, self.k)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        topk_probs = F.softmax(topk_logits, dim=-1)\n",
    "\n",
    "        # Sample from the top-k probability distribution\n",
    "        next_token_idx = torch.multinomial(topk_probs, num_samples=1)\n",
    "\n",
    "        # Map back to original vocabulary index and return\n",
    "        return topk_indices[next_token_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841cc701-acdc-4331-a319-c35151094894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token = 'z'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sampler = TopKSampler(3)\n",
    "vocabulary = list(set(\"abcdefghijklmnopqrstuvwxyz.\"))\n",
    "logits = torch.randn(1, 27)\n",
    "next_token_id = sampler(logits.squeeze(0))\n",
    "next_token = vocabulary[next_token_id.item()]\n",
    "print(f\"{next_token = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd4b7b-dca4-402c-9d66-58f859aaa5b4",
   "metadata": {},
   "source": [
    "# Top-P Sampling (Nucleus Sampling)\n",
    "\n",
    "Top-p sampling selects tokens from the smallest set of tokens whose cumulative probability mass exceeds a threshold $p$\n",
    "p (e.g., 0.75). Instead of restricting to a fixed top-k tokens, it dynamically chooses how many tokens to consider based on their cumulative probability.\n",
    "\n",
    "- Sort tokens by probability.\n",
    "- Take the top tokens until their cumulative probability $\\geq p$\n",
    "- Sample the next token from this reduced set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e188a9a-7ec7-4aca-a8ea-c178648ecc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopPSampler(Sampler):\n",
    "    def __init__(self, p=0.75):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, logits):\n",
    "        # Convert logits to probailities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sort probabilities in descending order\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "        # Compute cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Find the cutoff index where cumulative prop exceeds p\n",
    "        cutoff_index = torch.searchsorted(cumulative_probs, self.p)\n",
    "\n",
    "        # Mask tokens beyond cutoff index by setting their probabilities \n",
    "        # to 0 so they can't be sampled\n",
    "        filtered_probs = sorted_probs.clone()\n",
    "        filtered_probs[cutoff_index + 1:] = 0\n",
    "\n",
    "        # Normalize filtered probabilities\n",
    "        filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "\n",
    "        # Sample from filtered distribution\n",
    "        next_token_index = torch.multinomial(filtered_probs, num_samples=1)\n",
    "\n",
    "        # Map back to original token index and return\n",
    "        return sorted_indices[next_token_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7992a8-1e69-4168-a7b5-70728658c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token = 'e'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sampler = TopPSampler(0.7)\n",
    "vocabulary = list(set(\"abcdefghijklmnopqrstuvwxyz.\"))\n",
    "logits = torch.randn(1, 27)\n",
    "next_token_id = sampler(logits.squeeze(0))\n",
    "next_token = vocabulary[next_token_id.item()]\n",
    "print(f\"{next_token = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85444d7d-706c-41aa-a0b5-18ed55a4ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
